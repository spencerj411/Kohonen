{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e8e69c-b236-4d01-9f78-0f705b8c915a",
   "metadata": {
    "id": "81e8e69c-b236-4d01-9f78-0f705b8c915a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation</h1>\n",
    "\n",
    "In this blog post, we will specifically deal with keypoints estimation of **dogs** and show you how to fine-tune the very popular **YOLOv8** pose models from Ultralytics.\n",
    "\n",
    "<img src = \"https://learnopencv.com/wp-content/uploads/2023/09/yolov8m-predictions.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e0ca9-3848-458a-9dc1-84fd04e73706",
   "metadata": {
    "id": "c97e0ca9-3848-458a-9dc1-84fd04e73706"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1 The Stanford Dogs Dataset](#1-The-Stanford-Dogs-Dataset)\n",
    "* [2 Download Image Data and Keypoint Metadata](#2-Download-Image-Data-and-Keypoint-Metadata)\n",
    "* [3 Create YOLO Train and Valid Directories](#3-Create-YOLO-Train-and-Valid-Directories)\n",
    "* [4 Data Visualization](#4-Data-Visualization)\n",
    "* [5 Configurations](#5-Configurations)\n",
    "* [6 Training](#6-Training)\n",
    "* [7 Evaluation](#7-Evaluation)\n",
    "* [8 Predictions](#8-Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jvDsL6sIPM3s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1741999436262,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "jvDsL6sIPM3s",
    "outputId": "45591de1-2d3e-43f0-fe40-beb9c6d7e830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "JQ82vmZuPK0V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29806,
     "status": "ok",
     "timestamp": 1741999466069,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "JQ82vmZuPK0V",
    "outputId": "57585543-b849-4208-f47a-3d933b8c3f9f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# New Cell: Mount Drive and Set DRIVE_ROOT\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m, force_remount=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# New Cell: Mount Drive and Set DRIVE_ROOT\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "DRIVE_ROOT = \"/content/drive/My Drive/Projects/AI Dog Trainer\"\n",
    "print(f\"DRIVE_ROOT set to: {DRIVE_ROOT}\")\n",
    "print(os.listdir('/content/drive/My Drive/Projects/AI Dog Trainer/Datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa62b33c-73d8-43bb-a988-cf3eb33e4de6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85238,
     "status": "ok",
     "timestamp": 1741999551306,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "fa62b33c-73d8-43bb-a988-cf3eb33e4de6",
    "outputId": "c3a72525-7777-44a9-dd82-50cf7312d4da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in ./yolov8_venv/lib/python3.9/site-packages (8.3.91)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (3.9.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in ./yolov8_venv/lib/python3.9/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./yolov8_venv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./yolov8_venv/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./yolov8_venv/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./yolov8_venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./yolov8_venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./yolov8_venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./yolov8_venv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./yolov8_venv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./yolov8_venv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./yolov8_venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./yolov8_venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./yolov8_venv/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics -q\n",
    "!pip install --upgrade ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d691f3-1431-486a-aed7-fead57df9a3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14514,
     "status": "ok",
     "timestamp": 1741999565822,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "f2d691f3-1431-486a-aed7-fead57df9a3c",
    "outputId": "fc4adc5d-be03-49a8-ae9c-e7d074abd0fb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from shutil import copyfile\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1",
   "metadata": {
    "id": "8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1"
   },
   "source": [
    "## 1 The Stanford Dogs Dataset\n",
    "\n",
    "For our experiments, we will use the **Stanford Dataset**, which contains 120 breeds of dogs across **20,580** images. Besides, the dataset also contains the bounding box annotations for these images.\n",
    "\n",
    "However, the keypoint annotations need to be downloaded from the **StandfordExtra** dataset by filling up a **[google form](https://forms.gle/sRtbicgxsWvRtRmUA)**. The keypoint annotations are provided across **12,538** images for `20` keypoints of dog pose (`3` for each leg, `2` for each ear, `2` for the tail, nose, and jaw).\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/09/animal-pose-estimation-dog-kpts.png\" width=700>\n",
    "\n",
    "The authors have also provided keypoint metadata in the form of a CSV file containing the animal pose name, the color coding for each keypoint, etc. It, however, contains the info across 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55a89f-5d0f-412a-a46d-719ad4e9a485",
   "metadata": {
    "id": "2c55a89f-5d0f-412a-a46d-719ad4e9a485"
   },
   "source": [
    "## 2 Download Image Data and Keypoint Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4e95c-d726-4122-a328-243aacccb6c2",
   "metadata": {
    "id": "66c4e95c-d726-4122-a328-243aacccb6c2"
   },
   "source": [
    "The `download_and_unzip` utility downloads and extracts the **`images.tar`** file containing the images. Besides, we shall also download the **`keypoint_definitions.csv`** containing the keypoint metadata, such as the animal pose name, color coding for each keypoint, etc., across all 24  keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30414c02-3d41-4d1b-a8b4-5b98a8da6cce",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1741999565824,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "30414c02-3d41-4d1b-a8b4-5b98a8da6cce"
   },
   "outputs": [],
   "source": [
    "# Download and dataset.\n",
    "def download_and_unzip(url, save_path):\n",
    "\n",
    "    print(\"Downloading and extracting assets...\", end=\"\")\n",
    "    file = requests.get(url)\n",
    "    open(save_path, \"wb\").write(file.content)\n",
    "\n",
    "    try:\n",
    "        # Extract tarfile.\n",
    "        if save_path.endswith(\".tar\"):\n",
    "            with tarfile.open(save_path, \"r\") as tar:\n",
    "                tar.extractall(os.path.split(save_path)[0])\n",
    "\n",
    "        print(\"Done\")\n",
    "    except:\n",
    "        print(\"Invalid file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43929d-4d80-4c45-a7be-4a6685e99f3f",
   "metadata": {
    "id": "0c43929d-4d80-4c45-a7be-4a6685e99f3f"
   },
   "source": [
    "All the downloaded images are extracted to the Images  directory. It has the following directory structure:\n",
    "\n",
    "```python\n",
    "Images/\n",
    "├── n02085620-Chihuahua\n",
    "│   ├── n02085620_10074.jpg\n",
    "│   ├── n02085620_10131.jpg\n",
    "│   └── ...\n",
    "├── n02085782-Japanese_spaniel\n",
    "│   ├── n02085782_1039.jpg\n",
    "│   ├── n02085782_1058.jpg\n",
    "│   └── n02085782_962.jpg\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed3126-089f-44ab-8724-59bf86108480",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194691,
     "status": "ok",
     "timestamp": 1741999760516,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "c0ed3126-089f-44ab-8724-59bf86108480",
    "outputId": "8e406373-b250-4239-fcc3-0edf8cd9e6ca"
   },
   "outputs": [],
   "source": [
    "IMAGES_URL = r\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
    "IMAGES_DIR = \"Images\"\n",
    "IMAGES_TAR_PATH = os.path.join(os.getcwd(), f\"{IMAGES_DIR}.tar\")\n",
    "\n",
    "ANNS_METADATA_URL = r\"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n",
    "ANNS_METADATA = \"keypoint_definitions.csv\"\n",
    "\n",
    "# Download if dataset does not exists.\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    download_and_unzip(IMAGES_URL, IMAGES_TAR_PATH)\n",
    "    os.remove(IMAGES_TAR_PATH)\n",
    "\n",
    "if not os.path.isfile(ANNS_METADATA):\n",
    "    download_and_unzip(ANNS_METADATA_URL, ANNS_METADATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd54ce2-5830-4c21-8b61-65f27d5742e7",
   "metadata": {
    "id": "1dd54ce2-5830-4c21-8b61-65f27d5742e7"
   },
   "source": [
    "The annotations downloaded after filling out the form mentioned earlier are maintained in the `StanfordExtra_V12` directory which contains the annotation **JSON** file: `StanfordExtra_v12.json` contain the following structure:\n",
    "\n",
    "```python\n",
    "StanfordExtra_V12\n",
    "├── StanfordExtra_v12.json\n",
    "├── test_stanford_StanfordExtra_v12.npy\n",
    "├── train_stanford_StanfordExtra_v12.npy\n",
    "└── val_stanford_StanfordExtra_v12.npy\n",
    "```\n",
    "\n",
    "The train, validation, and test splits are provided as indices from the original **`StanfordExtra_v12.json`** data.\n",
    "\n",
    "The train, validation, and test sets contain annotations for **6773**, **4062**, and **1703** images, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177139c-ed62-48db-8f07-583da0546f72",
   "metadata": {
    "executionInfo": {
     "elapsed": 1928,
     "status": "ok",
     "timestamp": 1741999762447,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "7177139c-ed62-48db-8f07-583da0546f72"
   },
   "outputs": [],
   "source": [
    "ANN_PATH = f\"{DRIVE_ROOT}/Datasets/StanfordExtra_V12\"\n",
    "JSON_PATH = os.path.join(ANN_PATH, \"StanfordExtra_v12.json\")\n",
    "\n",
    "with open(JSON_PATH) as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5258f50-139a-40cf-8172-0a20a0ad7f3d",
   "metadata": {
    "id": "f5258f50-139a-40cf-8172-0a20a0ad7f3d"
   },
   "source": [
    "The files: **`train_stanford_StanfordExtra_v12.npy`** and **`test_stanford_StanfordExtra_v12.npy`** consist of the training and validation indices with respect to the original json_data list.\n",
    "\n",
    "For simplicity, we shall use the test data for validation. The training and the test sets comprise **6773** and **1703** samples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332c0b6-e228-44f9-878b-081f28d822dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1486,
     "status": "ok",
     "timestamp": 1741999763934,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "a332c0b6-e228-44f9-878b-081f28d822dd",
    "outputId": "13acf644-ca4c-40de-a8e6-718e33d0b734"
   },
   "outputs": [],
   "source": [
    "train_ids = np.load(os.path.join(ANN_PATH,\n",
    "                                 \"train_stanford_StanfordExtra_v12.npy\"))\n",
    "val_ids = np.load(os.path.join(ANN_PATH,\n",
    "                               \"test_stanford_StanfordExtra_v12.npy\"))\n",
    "\n",
    "print(f\"Train Samples: {len(train_ids)}\")\n",
    "print(f\"Validation Samples: {len(val_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89f50b-e951-40ab-9f44-68e68f731347",
   "metadata": {
    "id": "ce89f50b-e951-40ab-9f44-68e68f731347"
   },
   "source": [
    "## 3 Create YOLO Train and Valid Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e6114-d72d-44f9-8da5-57fde95db878",
   "metadata": {
    "id": "012e6114-d72d-44f9-8da5-57fde95db878"
   },
   "source": [
    "We will maintain the following directory structure for YOLOv8 dataset:\n",
    "\n",
    "```python\n",
    "animal-pose-data\n",
    "├── train\n",
    "│   ├── images (6773 files)\n",
    "│   └── labels (6773 files)\n",
    "└── valid\n",
    "    ├── images (1703 files)\n",
    "    └── labels (1703 files)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741999763936,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"animal-pose-data\"\n",
    "\n",
    "TRAIN_DIR         = f\"train\"\n",
    "TRAIN_FOLDER_IMG    = f\"images\"\n",
    "TRAIN_FOLDER_LABELS = f\"labels\"\n",
    "\n",
    "TRAIN_IMG_PATH   = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_IMG)\n",
    "TRAIN_LABEL_PATH = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_LABELS)\n",
    "\n",
    "VALID_DIR           = f\"valid\"\n",
    "VALID_FOLDER_IMG    = f\"images\"\n",
    "VALID_FOLDER_LABELS = f\"labels\"\n",
    "\n",
    "VALID_IMG_PATH   = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_IMG)\n",
    "VALID_LABEL_PATH = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_LABELS)\n",
    "\n",
    "os.makedirs(TRAIN_IMG_PATH, exist_ok=True)\n",
    "os.makedirs(TRAIN_LABEL_PATH, exist_ok=True)\n",
    "os.makedirs(VALID_IMG_PATH, exist_ok=True)\n",
    "os.makedirs(VALID_LABEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107901d-b191-4e67-b7a2-f89cd7b13a02",
   "metadata": {
    "id": "3107901d-b191-4e67-b7a2-f89cd7b13a02"
   },
   "source": [
    "Next, we will use `train_ids` and `val_ids` to gather the image and annotation data using `json_data` obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786dc4a-13bd-407d-9948-317c897068c8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741999763937,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "3786dc4a-13bd-407d-9948-317c897068c8"
   },
   "outputs": [],
   "source": [
    "train_json_data = []\n",
    "for train_id in train_ids:\n",
    "    train_json_data.append(json_data[train_id])\n",
    "\n",
    "val_json_data = []\n",
    "for val_id in val_ids:\n",
    "    val_json_data.append(json_data[val_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fbe67-9a16-4fed-bcad-80289bb83d2c",
   "metadata": {
    "id": "883fbe67-9a16-4fed-bcad-80289bb83d2c"
   },
   "source": [
    "### 3.1 Copy Image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fd4e1-58a1-4af0-90f3-4392025356a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1145,
     "status": "ok",
     "timestamp": 1741999765082,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "c82fd4e1-58a1-4af0-90f3-4392025356a7"
   },
   "outputs": [],
   "source": [
    "for data in train_json_data:\n",
    "    img_file = data[\"img_path\"]\n",
    "    filename = img_file.split(\"/\")[-1]\n",
    "    copyfile(os.path.join(IMAGES_DIR, img_file),\n",
    "             os.path.join(TRAIN_IMG_PATH, filename))\n",
    "\n",
    "\n",
    "for data in val_json_data:\n",
    "    img_file = data[\"img_path\"]\n",
    "    filename = img_file.split(\"/\")[-1]\n",
    "    copyfile(os.path.join(IMAGES_DIR, img_file),\n",
    "             os.path.join(VALID_IMG_PATH, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc611e8-9105-4392-823c-4ff12256f5e4",
   "metadata": {
    "id": "3dc611e8-9105-4392-823c-4ff12256f5e4"
   },
   "source": [
    "### 3.2 Create YOLO Annotation TXT FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d036a60-0f0e-4652-8f5d-fd7d101a9f76",
   "metadata": {
    "id": "1d036a60-0f0e-4652-8f5d-fd7d101a9f76"
   },
   "source": [
    "Our final task for data preparation is to create the boxes and the keypoint annotations in accordance with Ultralytics’ YOLO. Since we will deal with a single class (i.e., dogs), we set the class index to **`0`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d75bd-e340-4696-9d9b-97381ae1fd25",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1741999765086,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "b11d75bd-e340-4696-9d9b-97381ae1fd25"
   },
   "outputs": [],
   "source": [
    "CLASS_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec",
   "metadata": {
    "id": "4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec"
   },
   "source": [
    "The function **`create_yolo_boxes_kpts`** performs the following tasks:\n",
    "\n",
    "* Modifies visibility indicators for keypoints (setting the visibilities for labeled keypoints to 2).\n",
    "* Normalizes the coordinates of both bounding boxes and keypoints relative to the image dimensions.\n",
    "* Converts bounding boxes to $[x_{center}, \\ , y_{center},\\ width,\\ height]$ in normalized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2871243-41d0-4a36-92c1-17d4f250fbcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1741999765146,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "c2871243-41d0-4a36-92c1-17d4f250fbcf"
   },
   "outputs": [],
   "source": [
    "def create_yolo_boxes_kpts(img_size, boxes, lm_kpts):\n",
    "\n",
    "    IMG_W, IMG_H = img_size\n",
    "    # Modify kpts with visibilities as 1s to 2s.\n",
    "    vis_ones = np.where(lm_kpts[:, -1] == 1.)\n",
    "    lm_kpts[vis_ones, -1] = 2.\n",
    "\n",
    "    # Normalizing factor for bboxes and kpts.\n",
    "    res_box_array = np.array([IMG_W, IMG_H, IMG_W, IMG_H])\n",
    "    res_lm_array = np.array([IMG_W, IMG_H])\n",
    "\n",
    "    # Normalize landmarks in the range [0,1].\n",
    "    norm_kps_per_img = lm_kpts.copy()\n",
    "    norm_kps_per_img[:, :-1]  = norm_kps_per_img[:, :-1] / res_lm_array\n",
    "\n",
    "    # Normalize bboxes in the range [0,1].\n",
    "    norm_bbox_per_img = boxes / res_box_array\n",
    "\n",
    "    # Create bboxes coordinates to YOLO.\n",
    "    # x_c, y_c = x_min + bbox_w/2. , y_min + bbox_h/2.\n",
    "    yolo_boxes = norm_bbox_per_img.copy()\n",
    "    yolo_boxes[:2] = norm_bbox_per_img[:2] + norm_bbox_per_img[2:]/2.\n",
    "\n",
    "    return yolo_boxes, norm_kps_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4abe0-3467-4575-97d6-51e4c1226b73",
   "metadata": {
    "id": "fda4abe0-3467-4575-97d6-51e4c1226b73"
   },
   "source": [
    "We will finally create the `txt` files for YOLO based on the `train_json_data` and `val_json_data` obtained earlier. The function `create_yolo_txt_files` creates the required `txt` annotations in YOLO using the `create_yolo_boxes_kpts` utility function explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fa0ee-d9df-4d4f-8493-012a39959bcb",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1741999765147,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "6f7fa0ee-d9df-4d4f-8493-012a39959bcb"
   },
   "outputs": [],
   "source": [
    "def create_yolo_txt_files(json_data, LABEL_PATH):\n",
    "\n",
    "    for data in json_data:\n",
    "\n",
    "        IMAGE_ID = data[\"img_path\"].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        IMG_WIDTH, IMG_HEIGHT = data[\"img_width\"], data[\"img_height\"]\n",
    "\n",
    "        landmark_kpts  = np.nan_to_num(np.array(data[\"joints\"], dtype=np.float32))\n",
    "        landmarks_bboxes = np.array(data[\"img_bbox\"], dtype=np.float32)\n",
    "\n",
    "        bboxes_yolo, kpts_yolo = create_yolo_boxes_kpts(\n",
    "                                            (IMG_WIDTH, IMG_HEIGHT),\n",
    "                                            landmarks_bboxes,\n",
    "                                            landmark_kpts)\n",
    "\n",
    "        TXT_FILE = IMAGE_ID+\".txt\"\n",
    "\n",
    "        with open(os.path.join(LABEL_PATH, TXT_FILE), \"w\") as f:\n",
    "\n",
    "            x_c_norm, y_c_norm, box_width_norm, box_height_norm = round(bboxes_yolo[0],5),\\\n",
    "                                                                  round(bboxes_yolo[1],5),\\\n",
    "                                                                  round(bboxes_yolo[2],5),\\\n",
    "                                                                  round(bboxes_yolo[3],5),\\\n",
    "\n",
    "            kps_flattend = [round(ele,5) for ele in kpts_yolo.flatten().tolist()]\n",
    "            line = f\"{CLASS_ID} {x_c_norm} {y_c_norm} {box_width_norm} {box_height_norm} \"\n",
    "            line+= \" \".join(map(str, kps_flattend))\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb9989-e536-45d5-b198-771a16ce3025",
   "metadata": {
    "id": "9bbb9989-e536-45d5-b198-771a16ce3025"
   },
   "source": [
    "Finnally, we create the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6a84a-5678-475d-b95a-3b2b6ccba51c",
   "metadata": {
    "executionInfo": {
     "elapsed": 2211,
     "status": "ok",
     "timestamp": 1741999767355,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "a0c6a84a-5678-475d-b95a-3b2b6ccba51c"
   },
   "outputs": [],
   "source": [
    "create_yolo_txt_files(train_json_data, TRAIN_LABEL_PATH)\n",
    "create_yolo_txt_files(val_json_data, VALID_LABEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98afac-c5cf-4347-985c-6286362316dd",
   "metadata": {
    "id": "aa98afac-c5cf-4347-985c-6286362316dd"
   },
   "source": [
    "## 4 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f3b5e-5554-4309-8992-0ef691aa93ee",
   "metadata": {
    "id": "010f3b5e-5554-4309-8992-0ef691aa93ee"
   },
   "source": [
    "Before visualizing the samples, we can map the `hexadecimal` color codings available with **`keypoint_definitions.csv`** to RGB values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb68085-e8ac-49d2-a319-83f6d0fad95a",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1741999767371,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "feb68085-e8ac-49d2-a319-83f6d0fad95a"
   },
   "outputs": [],
   "source": [
    "ann_meta_data = pd.read_csv(\"keypoint_definitions.csv\")\n",
    "COLORS = ann_meta_data[\"Hex colour\"].values.tolist()\n",
    "\n",
    "COLORS_RGB_MAP = []\n",
    "for color in COLORS:\n",
    "    R, G, B = int(color[:2], 16), int(color[2:4], 16), int(color[4:], 16)\n",
    "    COLORS_RGB_MAP.append({color: (R,G,B)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d8a97-a77b-4e37-a51f-dff797ec73ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1741999767372,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "605d8a97-a77b-4e37-a51f-dff797ec73ab",
    "outputId": "6bcc5160-4489-461e-dea5-1eb5169bcece"
   },
   "outputs": [],
   "source": [
    "train_images = os.listdir(TRAIN_IMG_PATH)\n",
    "valid_images = os.listdir(VALID_IMG_PATH)\n",
    "\n",
    "print(f\"Training images: {len(train_images)}, Validation Images: {len(valid_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b31f59-c466-42a7-87a6-68a0c9a0c82f",
   "metadata": {
    "id": "78b31f59-c466-42a7-87a6-68a0c9a0c82f"
   },
   "source": [
    "The `draw_landmarks` function is used to annotate the corresponding landmark points on the image using COLORS_RGB_MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7df6-4a58-4e85-ab41-ebd533278f8e",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1741999767372,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "f99e7df6-4a58-4e85-ab41-ebd533278f8e"
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks):\n",
    "\n",
    "    radius = 5\n",
    "    # Check if image width is greater than 1000 px.\n",
    "    # To improve visualization.\n",
    "    if (image.shape[1] > 1000):\n",
    "        radius = 8\n",
    "\n",
    "    for idx, kpt_data in enumerate(landmarks):\n",
    "\n",
    "        loc_x, loc_y = kpt_data[:2].astype(\"int\").tolist()\n",
    "        color_id = list(COLORS_RGB_MAP[int(kpt_data[-1])].values())[0]\n",
    "\n",
    "        cv2.circle(image,\n",
    "                   (loc_x, loc_y),\n",
    "                   radius,\n",
    "                   color=color_id[::-1],\n",
    "                   thickness=-1,\n",
    "                   lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4535983-422b-480e-bb3d-2a79dd028eca",
   "metadata": {
    "id": "e4535983-422b-480e-bb3d-2a79dd028eca"
   },
   "source": [
    "The `draw_boxes` function is used to annotate the bounding boxes along with the confidence scores (if passed) on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb17095-84bb-499e-aea4-4fc9c4e67d0f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741999767372,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "7cb17095-84bb-499e-aea4-4fc9c4e67d0f"
   },
   "outputs": [],
   "source": [
    "def draw_boxes(image, detections, class_name = \"dog\", score=None, color=(0,255,0)):\n",
    "\n",
    "    font_size = 0.25 + 0.07 * min(image.shape[:2]) / 100\n",
    "    font_size = max(font_size, 0.5)\n",
    "    font_size = min(font_size, 0.8)\n",
    "    text_offset = 3\n",
    "\n",
    "    thickness = 2\n",
    "    # Check if image width is greater than 1000 px.\n",
    "    # To improve visualization.\n",
    "    if (image.shape[1] > 1000):\n",
    "        thickness = 10\n",
    "\n",
    "    xmin, ymin, xmax, ymax = detections[:4].astype(\"int\").tolist()\n",
    "    conf = round(float(detections[-1]),2)\n",
    "    cv2.rectangle(image,\n",
    "                  (xmin, ymin),\n",
    "                  (xmax, ymax),\n",
    "                  color=(0,255,0),\n",
    "                  thickness=thickness,\n",
    "                  lineType=cv2.LINE_AA)\n",
    "\n",
    "    display_text = f\"{class_name}\"\n",
    "\n",
    "    if score is not None:\n",
    "        display_text+=f\": {score:.2f}\"\n",
    "\n",
    "    (text_width, text_height), _ = cv2.getTextSize(display_text,\n",
    "                                                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                   font_size, 2)\n",
    "\n",
    "    cv2.rectangle(image,\n",
    "                      (xmin, ymin),\n",
    "                      (xmin + text_width + text_offset, ymin - text_height - int(15 * font_size)),\n",
    "                      color=color, thickness=-1)\n",
    "\n",
    "    image = cv2.putText(\n",
    "                    image,\n",
    "                    display_text,\n",
    "                    (xmin + text_offset, ymin - int(10 * font_size)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_size,\n",
    "                    (0, 0, 0),\n",
    "                    2, lineType=cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a9245-e884-4dec-b667-168717117b9e",
   "metadata": {
    "id": "283a9245-e884-4dec-b667-168717117b9e"
   },
   "source": [
    "The `visualize_annotations` is used to annotate both the bounding box coordinates and the landmark keypoints on the corresponding image after converting them to absoulute coordinates.\n",
    "\n",
    "Recall that both the bounding box coordinates and the keypoints were normalized in the range `[0, 1]`. However, to plot them, we need the absolute coordinates.\n",
    "\n",
    "The conversion mapping from YOLO bboxes to $[x_{min}, y_{min}, x_{max}, y_{max}]$ is pretty straight forward and can be obtained using the following set of equations:\n",
    "\n",
    "$$x_{min} = \\frac{W}{2} (2x_{center} \\ - \\ width)$$\n",
    "\n",
    "$$y_{min} = \\frac{H}{2} (2y_{center} \\ - \\ height)$$\n",
    "\n",
    "$$x_{max} = x_{min} + width * W$$\n",
    "\n",
    "$$y_{max} = y_{min} + height * H$$\n",
    "\n",
    "\n",
    "Similarly, the keypoints can denormalized (to the absolute coordinates) using:\n",
    "\n",
    "$$x_{abs} = x_{norm}* W$$\n",
    "\n",
    "$$y_{abs} = y_{norm}* H$$\n",
    "\n",
    "\n",
    "Here, the `width` and `height` are the box width and height respectively; whereas `W` and `H` are the image width and height respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a22ae9-59bc-468e-8217-0c5cc67a75dc",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1741999767382,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "d2a22ae9-59bc-468e-8217-0c5cc67a75dc"
   },
   "outputs": [],
   "source": [
    "def visualize_annotations(image, box_data, keypoints_data):\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    shape_multiplier = np.array(image.shape[:2][::-1]) # (W, H).\n",
    "    # Final absolute coordinates (xmin, ymin, xmax, ymax).\n",
    "    denorm_boxes = np.zeros_like(box_data)\n",
    "\n",
    "    # De-normalize center coordinates from YOLO to (xmin, ymin).\n",
    "    denorm_boxes[:, :2] = (shape_multiplier/2.) * (2*box_data[:,:2] - box_data[:,2:])\n",
    "\n",
    "    # De-normalize width and height from YOLO to (xmax, ymax).\n",
    "    denorm_boxes[:, 2:] = denorm_boxes[:,:2] + box_data[:,2:]*shape_multiplier\n",
    "\n",
    "    for boxes, kpts in zip(denorm_boxes, keypoints_data):\n",
    "        # De-normalize landmark coordinates.\n",
    "        kpts[:, :2]*= shape_multiplier\n",
    "        image = draw_boxes(image, boxes)\n",
    "        image = draw_landmarks(image, kpts)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084158f-5e58-419b-8cfc-40fab86839ae",
   "metadata": {
    "id": "b084158f-5e58-419b-8cfc-40fab86839ae"
   },
   "source": [
    "The following plot shows a few image samples with their corresponding ground truth annotation. The keypoint annotations are filtered based on their corresponding visibility flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aa95f-33d7-43c3-868a-826324258898",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659,
     "output_embedded_package_id": "1iI0pc0M4cU5znV0ItkdNMTA70qkO9j7j"
    },
    "executionInfo": {
     "elapsed": 4046,
     "status": "ok",
     "timestamp": 1741999771422,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "326aa95f-33d7-43c3-868a-826324258898",
    "outputId": "8df8dda1-023a-455c-e208-5118d14ec34a"
   },
   "outputs": [],
   "source": [
    "IMAGE_FILES = os.listdir(TRAIN_IMG_PATH)\n",
    "NUM_LANDMARKS = 24\n",
    "\n",
    "num_samples = 8\n",
    "num_rows = 2\n",
    "num_cols = num_samples//num_rows\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=num_cols,\n",
    "        figsize=(25, 15),\n",
    "    )\n",
    "\n",
    "random.seed(45)\n",
    "random.shuffle(IMAGE_FILES)\n",
    "\n",
    "for idx, (file, axis) in enumerate(zip(IMAGE_FILES[:num_samples], ax.flat)):\n",
    "\n",
    "    image = cv2.imread(os.path.join(TRAIN_IMG_PATH, file))\n",
    "\n",
    "    # Obtain the txt file for the corresponding image file.\n",
    "    filename = file.split(\".\")[0]\n",
    "    # Split each object instance in separate lists.\n",
    "    with open(os.path.join(TRAIN_LABEL_PATH, filename+\".txt\"), \"r\") as file:\n",
    "        label_data = [x.split() for x in file.read().strip().splitlines() if len(x)]\n",
    "\n",
    "    label_data = np.array(label_data, dtype=np.float32)\n",
    "\n",
    "    # YOLO BBox instances in [x-center, y-center, width, height] in normalized form.\n",
    "    box_instances = label_data[:,1:5]\n",
    "    # Shape: (N, 4), where, N = #instances per-image\n",
    "\n",
    "    # Kpt instances.\n",
    "    # Filter keypoints based on visibility.\n",
    "    instance_kpts = []\n",
    "    kpts_data = label_data[:,5:].reshape(-1, NUM_LANDMARKS, 3)\n",
    "\n",
    "    for inst_kpt in kpts_data:\n",
    "        vis_ids = np.where(inst_kpt[:, -1]>0.)[0]\n",
    "        vis_kpts = inst_kpt[vis_ids][:,:2]\n",
    "        vis_kpts = np.concatenate([vis_kpts, np.expand_dims(vis_ids, axis=-1)], axis=-1)\n",
    "        instance_kpts.append(vis_kpts)\n",
    "\n",
    "    image_ann = visualize_annotations(image, box_instances, instance_kpts)\n",
    "    axis.imshow(image_ann[...,::-1])\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad=4., w_pad=4.)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a171a6-8a1d-4215-bbdf-a0c0bdde5c36",
   "metadata": {
    "id": "98a171a6-8a1d-4215-bbdf-a0c0bdde5c36"
   },
   "source": [
    "## 5 Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5071680-68cc-4240-9ef5-e6fc609ba156",
   "metadata": {
    "id": "e5071680-68cc-4240-9ef5-e6fc609ba156"
   },
   "source": [
    "### 5.1 Training Configuration\n",
    "\n",
    "We shall define the training configuration for fine-tuning in the `TrainingConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e754d5c-1be0-4238-b113-3a02ad8f25a6",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741999771433,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "7e754d5c-1be0-4238-b113-3a02ad8f25a6"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    DATASET_YAML:   str = \"animal-keypoints.yaml\"\n",
    "    MODEL:          str = \"yolov8s-pose.pt\"\n",
    "    EPOCHS:         int = 40 # could only get to 45 epochs with \"default\" training and data configs.\n",
    "    KPT_SHAPE:    tuple = (24,3)\n",
    "    PROJECT:        str = \"Animal_Keypoints\"\n",
    "    NAME:           str = f\"{MODEL.split('.')[0]}_{EPOCHS}_epochs\"\n",
    "    CLASSES_DICT:  dict = field(default_factory = lambda:{0 : \"dog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c05db-5e44-4574-9a23-e8fc00e7b099",
   "metadata": {
    "id": "120c05db-5e44-4574-9a23-e8fc00e7b099"
   },
   "source": [
    "### 5.2 Data Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473ee95-d50d-4cf3-b2de-e63d75b03895",
   "metadata": {
    "id": "3473ee95-d50d-4cf3-b2de-e63d75b03895"
   },
   "source": [
    "The `DatasetConfig` class takes in the various hyperparameters related to the data such as the image size and batch size to be used while training, along with the various augmentation probabilities such as Mosaic, horizontal flip, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769ab0c-7773-4fe4-a828-76ec17d3cff8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741999771434,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "1769ab0c-7773-4fe4-a828-76ec17d3cff8"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    IMAGE_SIZE:    int   = 416  # 640\n",
    "    BATCH_SIZE:    int   = 24  # 16\n",
    "    CLOSE_MOSAIC:  int   = 10\n",
    "    MOSAIC:        float = 0.4\n",
    "    FLIP_LR:       float = 0.0 # Turn off horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005212b5-80e9-4487-9f1f-3c9ec2de6fcc",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1741999771444,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "005212b5-80e9-4487-9f1f-3c9ec2de6fcc"
   },
   "outputs": [],
   "source": [
    "train_config = TrainingConfig()\n",
    "data_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43529c-85a0-4bb7-86e4-463bfab82d54",
   "metadata": {
    "id": "be43529c-85a0-4bb7-86e4-463bfab82d54"
   },
   "source": [
    "Before we start our training, we need to create a `yaml` containing the path to the images and label files. We also need to specify the class names, starting from index=0 and the keypoint shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c63d59-91b9-455a-92cc-355af5aa0dc5",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741999771446,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "77c63d59-91b9-455a-92cc-355af5aa0dc5"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "data_dict = dict(\n",
    "                path      = os.path.join(current_dir, DATA_DIR),\n",
    "                train     = os.path.join(TRAIN_DIR, TRAIN_FOLDER_IMG),\n",
    "                val       = os.path.join(VALID_DIR, VALID_FOLDER_IMG),\n",
    "                names     = train_config.CLASSES_DICT,\n",
    "                kpt_shape = list(train_config.KPT_SHAPE),\n",
    "               )\n",
    "\n",
    "with open(train_config.DATASET_YAML, \"w\") as config_file:\n",
    "    yaml.dump(data_dict, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7c435-78fb-4d76-aea0-335da96db326",
   "metadata": {
    "id": "13a7c435-78fb-4d76-aea0-335da96db326"
   },
   "source": [
    "## 6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fffd85-cccc-446e-b51a-b885991a2b6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3517616,
     "status": "ok",
     "timestamp": 1742003289062,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "54fffd85-cccc-446e-b51a-b885991a2b6f",
    "outputId": "f776f024-2d15-40a9-cf57-a8933bd5a14c"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "\n",
    "def monitor_gpu():\n",
    "    while True:\n",
    "        # Run nvidia-smi and capture output\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        # Wait 30 seconds before next check (adjust as needed)\n",
    "        time.sleep(30)\n",
    "\n",
    "# Start GPU monitoring in a background thread\n",
    "monitor_thread = threading.Thread(target=monitor_gpu, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "pose_model = model = YOLO(train_config.MODEL)\n",
    "\n",
    "pose_model.train(data    = train_config.DATASET_YAML,\n",
    "            epochs       = train_config.EPOCHS,\n",
    "            imgsz        = data_config.IMAGE_SIZE,\n",
    "            batch        = data_config.BATCH_SIZE,\n",
    "            project      = train_config.PROJECT,\n",
    "            name         = train_config.NAME,\n",
    "            close_mosaic = data_config.CLOSE_MOSAIC,\n",
    "            mosaic       = data_config.MOSAIC,\n",
    "            fliplr       = data_config.FLIP_LR\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e1adb-eaae-44f1-880a-e7544e7b6f6a",
   "metadata": {
    "id": "6d9e1adb-eaae-44f1-880a-e7544e7b6f6a"
   },
   "source": [
    "## 7 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e12a5-c16d-401a-98e8-f1cd45ac05f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24157,
     "status": "ok",
     "timestamp": 1742004296382,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "506e12a5-c16d-401a-98e8-f1cd45ac05f1",
    "outputId": "b2c40ae5-ef09-47dd-c60a-63a619c9a3c4"
   },
   "outputs": [],
   "source": [
    "ckpt_path  = os.path.join(train_config.PROJECT, train_config.NAME, \"weights\", \"best.pt\")\n",
    "model_pose = YOLO(ckpt_path)\n",
    "\n",
    "metrics = model_pose.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f18c87-bb0c-45f6-a499-0a2086073fe6",
   "metadata": {
    "id": "b8f18c87-bb0c-45f6-a499-0a2086073fe6"
   },
   "source": [
    "## 8 Predictions\n",
    "\n",
    "The `prepare_predictions` function obtains the predicted boxes, confidence scores, and keypoints for the corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f8f3b-263e-443b-944a-057b935c188a",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742004551563,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "802f8f3b-263e-443b-944a-057b935c188a"
   },
   "outputs": [],
   "source": [
    "def prepare_predictions(\n",
    "    image_dir_path,\n",
    "    image_filename,\n",
    "    model,\n",
    "    BOX_IOU_THRESH = 0.55,\n",
    "    BOX_CONF_THRESH=0.30,\n",
    "    KPT_CONF_THRESH=0.68):\n",
    "\n",
    "    image_path = os.path.join(image_dir_path, image_filename)\n",
    "    image = cv2.imread(image_path).copy()\n",
    "\n",
    "    results = model.predict(image_path, conf=BOX_CONF_THRESH, iou=BOX_IOU_THRESH)[0].cpu()\n",
    "\n",
    "    if not len(results.boxes.xyxy):\n",
    "        return image\n",
    "\n",
    "    # Get the predicted boxes, conf scores and keypoints.\n",
    "    pred_boxes = results.boxes.xyxy.numpy()\n",
    "    pred_box_conf = results.boxes.conf.numpy()\n",
    "    pred_kpts_xy = results.keypoints.xy.numpy()\n",
    "    pred_kpts_conf = results.keypoints.conf.numpy()\n",
    "\n",
    "    # Draw predicted bounding boxes, conf scores and keypoints on image.\n",
    "    for boxes, score, kpts, confs in zip(pred_boxes, pred_box_conf, pred_kpts_xy, pred_kpts_conf):\n",
    "        kpts_ids = np.where(confs > KPT_CONF_THRESH)[0]\n",
    "        filter_kpts = kpts[kpts_ids]\n",
    "        filter_kpts = np.concatenate([filter_kpts, np.expand_dims(kpts_ids, axis=-1)], axis=-1)\n",
    "        image = draw_boxes(image, boxes, score=score)\n",
    "        image = draw_landmarks(image, filter_kpts)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7de57-709d-4ae3-8345-f6673b4d34d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "error",
     "timestamp": 1742004556435,
     "user": {
      "displayName": "Spencer J",
      "userId": "14674098974875562379"
     },
     "user_tz": -660
    },
    "id": "97c7de57-709d-4ae3-8345-f6673b4d34d0",
    "outputId": "aa9e80ba-8b28-46aa-93d4-88d1f1d8c297"
   },
   "outputs": [],
   "source": [
    "VAL_IMAGE_FILES = os.listdir(VALID_IMG_PATH)\n",
    "\n",
    "num_samples = 9\n",
    "num_rows = 3\n",
    "num_cols = num_samples//num_rows\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=num_cols,\n",
    "        figsize=(25, 15),\n",
    "    )\n",
    "\n",
    "random.seed(90)\n",
    "random.shuffle(VAL_IMAGE_FILES)\n",
    "\n",
    "for idx, (file, axis) in enumerate(zip(VAL_IMAGE_FILES[:num_samples], ax.flat)):\n",
    "\n",
    "    image_pred = prepare_predictions(VALID_IMG_PATH, file, model_pose)\n",
    "    axis.imshow(image_pred[...,::-1])\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(h_pad=4., w_pad=4.)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2eeae",
   "metadata": {
    "id": "38b2eeae"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My Venv (Python 3)",
   "language": "python",
   "name": "yolov8_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
