{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organising Map Challenge\n",
    "\n",
    "## The Kohonen Network\n",
    "\n",
    "The Kohonen Self Organising Map (SOM) provides a data visualization technique which helps to understand high dimensional data by reducing the dimensions of data to a map. SOM also represents clustering concept by grouping similar data together.\n",
    "\n",
    "Unlike other learning technique in neural networks, training a SOM requires no target vector. A SOM learns to classify the training data without any external supervision.\n",
    "\n",
    "![Network](kohonen.png)\n",
    "\n",
    "### Structure\n",
    "A network has a width and a height that descibes the grid of nodes.  For example, the grid may be 4x4, and so there would be 16 nodes.\n",
    "\n",
    "Each node has a weight for each value in the input vector.  A weight is simply a float value that the node multiplies the input value by to determine how influential it is (see below)\n",
    "\n",
    "Each node has a set of weights that match the size of the input vector.  For example, if the input vector has 10 elements, each node would have 10 weights.\n",
    "\n",
    "### Training \n",
    "To train the network\n",
    "\n",
    "1. Each node's weights are initialized.\n",
    "2. We enumerate through the training data for some number of iterations (repeating if necessary).  The current value we are training against will be referred to as the `current input vector`\n",
    "3. Every node is examined to calculate which one's weights are most like the input vector. The winning node is commonly known as the Best Matching Unit (BMU).\n",
    "4. The radius of the neighbourhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice,  but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighbourhood.\n",
    "5. Each neighbouring node's (the nodes found in step 4) weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.\n",
    "6. Go to step 2 until we've completed N iterations.\n",
    "    \n",
    "\n",
    "### Calculating the Best Matching Unit (BMU)\n",
    "\n",
    "To determine the best matching unit, one method is to iterate through all the nodes and calculate the Euclidean distance between each node's weight vector and the current input vector. The node with a weight vector closest to the input vector is tagged as the BMU.\n",
    "\n",
    "The Euclidean distance $\\mathsf{distance}_{i}$ (from the input vector $V$ to the $i$th node's weights $W_i$)is given as (using Pythagoras):\n",
    "\n",
    "$$ \\mathsf{distance}_{i}=\\sqrt{\\sum_{k=0}^{k=n}(V_k - W_{i_k})^2}$$\n",
    "\n",
    "where V is the current input vector and $W_i$ is the node's weight vector.  $n$ is the size of the input & weight vector.\n",
    "\n",
    "*Note*: $V$ and $W$ are vectors.  $V$ is the input vector, and $W_i$ is the weight vector of the $i$th node.  $V_k$ and $W_{i_k}$ represent the $k$'th value within those vectors.  \n",
    "\n",
    "The BMU is the node with the minimal distance for the current input vector\n",
    "\n",
    "### Calculating the Neighbourhood Radius\n",
    "\n",
    "The next step is to calculate which of the other nodes are within the BMU's neighbourhood. All these nodes will have their weight vectors altered.\n",
    "\n",
    "First we calculate what the radius of the neighbourhood should be and then use Pythagoras to determine if each node is within the radial distance or not.\n",
    "\n",
    "A unique feature of the Kohonen learning algorithm is that the area of the neighbourhood shrinks over time. To do this we use the exponential decay function:\n",
    "\n",
    "Given a desired number of training iterations $n$:\n",
    "$$n_{\\mathsf{max iterations}} = 100$$\n",
    "\n",
    "Calculate the radius $\\sigma_t$ at iteration number $t$:\n",
    "\n",
    "$$\\sigma_t = \\sigma_0 \\exp\\left(- \\frac{t}{\\lambda} \\right) \\qquad t = 1,2,3,4... $$\n",
    "\n",
    "Where $\\sigma_0$ denotes the neighbourhood radius at iteration $t=0$, $t$ is the current iteration. We define $\\sigma_0$ (the initial radius) and $\\lambda$ (the time constant) as below:\n",
    "\n",
    "$$\\sigma_0 = \\frac{\\max(width,height)}{2} \\qquad \\lambda = \\frac{n_{\\mathsf{max iterations}}}{\\log(\\sigma_0)} $$\n",
    "\n",
    "Where $width$ & $height$ are the width and height of the grid.\n",
    "\n",
    "### Calculating the Learning Rate\n",
    "\n",
    "We define the initial leanring rate $\\alpha_0$ at iteration $t = 0$ as:\n",
    "$$\\alpha_0 = 0.1$$\n",
    "\n",
    "So, we can calculate the learning rate at a given iteration t as:\n",
    "\n",
    "$$\\alpha_t = \\alpha_0 \\exp \\left(- \\frac{t}{\\lambda} \\right) $$\n",
    "\n",
    "where $t$ is the iteration number, $\\lambda$ is the time constant (calculated above)\n",
    "        \n",
    "### Calculating the Influence\n",
    "\n",
    "As well as the learning rate, we need to calculate the influence $\\theta_t$ of the learning/training at a given iteration $t$.  \n",
    "\n",
    "So for each node, we need to caclulate the euclidean distance $d_i$ from the BMU to that node.  Similar to when we calculate the distance to find the BMU, we use Pythagoras.  The current ($i$th) node's x position is given by $x(W_i)$, and the BMU's x position is, likewise, given by $x(Z)$.  Similarly, $y()$ returns the y position of a node.\n",
    "\n",
    "$$ d_{i}=\\sqrt{(x(W_i) - x(Z))^2 + (y(W_i) - y(Z))^2} $$\n",
    "\n",
    "Then, the influence decays over time according to:\n",
    "\n",
    "$$\\theta_t = \\exp \\left( - \\frac{d_{i}^2}{2\\sigma_t^2} \\right) $$\n",
    "\n",
    "Where $\\sigma_t$ is the neighbourhood radius at iteration $t$ as calculated above. \n",
    "\n",
    "Note: You will need to come up with an approach to x() and y().\n",
    "\n",
    "\n",
    "### Updating the Weights\n",
    "\n",
    "To update the weights of a given node, we use:\n",
    "\n",
    "$$W_{i_{t+1}} = W_{i_t} + \\alpha_t \\theta_t (V_t - W_{i_t})$$\n",
    "        \n",
    "So $W_{i_{t+1}}$ is the new value of the weight for the $i$th node, $V_t$ is the current value of the training data, $W_{i_t}$ is the current weight and $\\alpha_t$ and $\\theta_t$ are the learning rate and influence calculated above.\n",
    "\n",
    "*Note*: the $W$ and $V$ are vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10x10 network after 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 100x100 network after 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Sam has written an implementation of a Self Organising Map. Consider the following criteria when assessing Sam's code:\n",
    "\n",
    "- Could the code be made more efficient? A literal interpretation of the instructions above is not necessary.\n",
    "- Is the code best structured for later use by other developers and in anticipation of productionisation?\n",
    "- How would you approach productionising this application?\n",
    "- Anything else you think is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kohonen.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train(input_data, n_max_iterations, width, height):\n",
    "    σ0 = max(width, height) / 2\n",
    "    α0 = 0.1\n",
    "    weights = np.random.random((width, height, 3))\n",
    "    λ = n_max_iterations / np.log(σ0)\n",
    "    for t in range(n_max_iterations):\n",
    "        σt = σ0 * np.exp(-t/λ)\n",
    "        αt = α0 * np.exp(-t/λ)\n",
    "        for vt in input_data:\n",
    "            bmu = np.argmin(np.sum((weights - vt) ** 2, axis=2))\n",
    "            bmu_x, bmu_y = np.unravel_index(bmu, (width, height))\n",
    "            for x in range(width):\n",
    "                for y in range(height):\n",
    "                    di = np.sqrt(((x - bmu_x) ** 2) + ((y - bmu_y) ** 2))\n",
    "                    θt = np.exp(-(di ** 2) / (2*(σt ** 2)))\n",
    "                    weights[x, y] += αt * θt * (vt - weights[x, y])\n",
    "    return weights\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Generate data\n",
    "    input_data = np.random.random((10,3))\n",
    "    image_data = train(input_data, 100, 10, 10)\n",
    "\n",
    "    plt.imsave('100.png', image_data)\n",
    "\n",
    "    # Generate data\n",
    "    input_data = np.random.random((10,3))\n",
    "    image_data = train(input_data, 1000, 100, 100)\n",
    "\n",
    "    plt.imsave('1000.png', image_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reponse to Challenge\n",
    "\n",
    "Initial Observations:\n",
    "- Data science libraries like NumPy are used in the implementation (good thing)\n",
    "- the training logic was defined as a function that accepts parameters, of which are well named (good thing)\n",
    "- function is decoupled from the rest of the script, can easily be copied and pasted (good thing)\n",
    "- there are 4 nested for loops (potentially a bad thing)\n",
    "- no data validation (bad thing)\n",
    "- no exception handling (bad thing)\n",
    "- some variable names use special characters (bad thing)\n",
    "- names of png files are hardcoded (bad thing)\n",
    "- no useful comments (bad thing) eg. no Docstrings\n",
    "\n",
    "Top 5 Improvement Recommendations:\n",
    "\n",
    " 1. (Efficiency) Leverage vectorisation to improve performance when calculating the distances from a BMU to rest of the nodes in the grid; using NumPy... (...or at least use syntactic sugar to lessen the lines of code; not for efficiency but for readability)\n",
    " 2. (Productionisation) implement data validation, exception handling, and general logging statements throughout the code.\n",
    " 3. (Structure) refactor variable names to names that are easier to type\n",
    " 4. (Structure) using OOP, implement SOM and its config as classes\n",
    " 5. (Efficiency) Potentially implement a \"patience\" mechanism that will stop the training if the weights have not changed after x iterations/below a certain tolerance value.\n",
    "\n",
    "Anything else?\n",
    "- Research if there exists a library, that is reliable (ie. widely used), that has already implemented a SOM model. Manually implementing is great for understanding concepts, but for production, reusing 'proven' code is the way to go.\n",
    "\n",
    "\n",
    "## AI Generated Code to Illustrate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Recommendation 2: Add logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Recommendation 4: Use OOP with config class\n",
    "class SOMConfig:\n",
    "    def __init__(self):\n",
    "        # Recommendation 3: Easier variable names\n",
    "        self.width = 10\n",
    "        self.height = 10\n",
    "        self.input_dim = 3\n",
    "        self.max_iter = 100\n",
    "        self.learn_rate = 0.1\n",
    "        self.tolerance = 0.001  # Used for patience\n",
    "        self.patience = 10\n",
    "\n",
    "# Recommendation 4: SOM as a class\n",
    "class SOM:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.weights = np.random.random((config.width, config.height, config.input_dim))\n",
    "\n",
    "    def train(self, data):\n",
    "        # Recommendation 2: Validate input and add exception handling\n",
    "        try:\n",
    "            if not isinstance(data, np.ndarray) or len(data.shape) != 2:\n",
    "                logger.error(\"Input must be a 2D NumPy array\")\n",
    "                raise ValueError(\"Input must be a 2D NumPy array\")\n",
    "            if data.shape[1] != self.config.input_dim:\n",
    "                logger.error(\"Input dim doesn't match weights\")\n",
    "                raise ValueError(\"Input dim must match weights\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Recommendation 2: Add comments\n",
    "        logger.info(\"Starting SOM training...\")\n",
    "        # Calculate initial radius and time constant for decay\n",
    "        radius = max(self.config.width, self.config.height) / 2\n",
    "        time_const = self.config.max_iter / np.log(radius)\n",
    "\n",
    "        # Recommendation 1: Precompute coordinates for vectorized distance calc\n",
    "        x, y = np.arange(self.config.width), np.arange(self.config.height)\n",
    "        xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "        for t in range(self.config.max_iter):\n",
    "            # Recommendation 2: Comment key steps\n",
    "            # Save old weights for patience check\n",
    "            old_weights = self.weights.copy()\n",
    "            curr_lr = self.config.learn_rate * np.exp(-t / time_const)\n",
    "            curr_radius = radius * np.exp(-t / time_const)\n",
    "\n",
    "            for sample in data:\n",
    "                # Find BMU\n",
    "                diff = self.weights - sample\n",
    "                dist = np.sum(diff ** 2, axis=2)\n",
    "                bmu_idx = np.argmin(dist)\n",
    "                bmu_x, bmu_y = divmod(bmu_idx, self.config.height)\n",
    "\n",
    "                # Recommendation 1: Vectorized distance calc\n",
    "                dist_to_bmu = np.sqrt((xx - bmu_x)**2 + (yy - bmu_y)**2)\n",
    "                influence = np.exp(-dist_to_bmu**2 / (2 * curr_radius**2))\n",
    "\n",
    "                # Update weights\n",
    "                update = curr_lr * influence[:, :, np.newaxis] * (sample - self.weights)\n",
    "                self.weights += update\n",
    "\n",
    "            # Recommendation 5: Patience mechanism\n",
    "            change = np.sum(np.abs(self.weights - old_weights))\n",
    "            if change < self.config.tolerance:\n",
    "                logger.info(f\"Weights stabilized at iteration {t} - stopping early\")\n",
    "                break\n",
    "\n",
    "        logger.info(\"Training complete\")\n",
    "        return self.weights\n",
    "\n",
    "# Test\n",
    "if __name__ == '__main__':\n",
    "    config = SOMConfig()\n",
    "    som = SOM(config)\n",
    "    data = np.random.random((10, 3))\n",
    "    weights = som.train(data)\n",
    "    print(\"Done. For production, use MiniSom.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv (Python 3)",
   "language": "python",
   "name": "yolov8_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
